{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.atmos.illinois.edu/~snesbitt/Illinois-Wordmark-Horizontal-Full-Color-RGB.svg\" width=300>\n",
    "# Converting RMA BUFR files to cfradial\n",
    "\n",
    "Steve Nesbitt\n",
    "\n",
    "Department of Atmospheric Sciences\n",
    "\n",
    "University of Illinois at Urbana-Champaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#don't execute this block if you're not using ipyparallel\n",
    "from ipyparallel import Client\n",
    "c = Client(profile='slurm')\n",
    "v = c[:]\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#comment the next line out if you are not using ipyparallel\n",
    "@v.parallel(block=True)\n",
    "\n",
    "def proc_files(pathname):\n",
    "    import os, fnmatch\n",
    "    #from time import clock\n",
    "    import sys, glob\n",
    "\n",
    "    # Configuration section\n",
    "\n",
    "    # bufr2hdf5 executable path (change for your system)\n",
    "    path_conv=\"/data/keeling/a/snesbitt/software/bbufr/tests/bufr2hdf5\"\n",
    "\n",
    "    # Tables path (change for your system)\n",
    "    path_tables = \"/data/keeling/a/snesbitt/python/RadarMeteo/conversores/RMA/bbufr/tables\"\n",
    "\n",
    "    #=============================================================================#\n",
    "\n",
    "    # List of BUFR file names\n",
    "    fname=glob.glob(os.path.join(pathname,'*/*.BUFR'))\n",
    "    print(os.path.join(pathname,'*/*.BUFR'))\n",
    "    for ii in range(0, len(fname)):\n",
    "\n",
    "        print(fname[ii])\n",
    "        input_file  = fname[ii]\n",
    "        output_file = fname[ii][:-5] + \".H5\"\n",
    "        if ~os.path.isfile(fname[ii]):\n",
    "            os.system(path_conv + \" -d \" + path_tables + \" \" + input_file + \" \" + output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob2\n",
    "#Create list of folders with filesystem wildcard\n",
    "folder = '/data/meso/a/snesbitt/RMA1-new/201?/??/??/??'\n",
    "files=np.array(glob2.glob(folder))\n",
    "proc_files.map(files)\n",
    "# If you don't have ipyparallel use the following instead\n",
    "# proc_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#comment the next line out if you are not using ipyparallel\n",
    "@v.parallel(block=True)\n",
    "\n",
    "def convert_h5_to_cfradial(path):\n",
    "\n",
    "    import datetime\n",
    "    import os\n",
    "    import numpy as np\n",
    "    try:\n",
    "        import h5py\n",
    "        _H5PY_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        _H5PY_AVAILABLE = False\n",
    "\n",
    "    from pyart.config import FileMetadata, get_fillvalue\n",
    "    from pyart.io.common import make_time_unit_str, _test_arguments\n",
    "    from pyart.core.radar import Radar\n",
    "    from pyart.exceptions import MissingOptionalDependency\n",
    "    from pyart.io import write_cfradial\n",
    "\n",
    "    import glob\n",
    "    \n",
    "    from netcdftime import utime\n",
    "\n",
    "    SINARAME_H5_FIELD_NAMES = {\n",
    "        'TH': 'total_power',        # uncorrected reflectivity, horizontal\n",
    "        'TV': 'total_power_vertical',        # uncorrected reflectivity, vertical\n",
    "        'DBZH': 'reflectivity',     # corrected reflectivity, horizontal\n",
    "        'DBZV': 'reflectivity',     # corrected reflectivity, vertical\n",
    "        'CM': 'clutter mask',\n",
    "        'ZDR': 'differential_reflectivity',     # differential reflectivity\n",
    "        'RHOHV': 'cross_correlation_ratio',\n",
    "        'LDR': 'linear_polarization_ratio',\n",
    "        'PHIDP': 'differential_phase',\n",
    "        'KDP': 'specific_differential_phase',\n",
    "        'SQI': 'normalized_coherent_power',\n",
    "        'SNR': 'signal_to_noise_ratio',\n",
    "        'VRAD': 'velocity',\n",
    "        'WRAD': 'spectrum_width',\n",
    "        'QIND': 'quality_index',\n",
    "    }\n",
    "\n",
    "    def read_sinarame_h5(filename, field_names=None, additional_metadata=None,\n",
    "                         file_field_names=False, exclude_fields=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Read a SINARAME_H5 file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str\n",
    "            Name of the SINARAME_H5 file to read.\n",
    "        field_names : dict, optional\n",
    "            Dictionary mapping SINARAME_H5 field names to radar field names. If a\n",
    "            data type found in the file does not appear in this dictionary or has\n",
    "            a value of None it will not be placed in the radar.fields dictionary.\n",
    "            A value of None, the default, will use the mapping defined in the\n",
    "            Py-ART configuration file.\n",
    "        additional_metadata : dict of dicts, optional\n",
    "            Dictionary of dictionaries to retrieve metadata from during this read.\n",
    "            This metadata is not used during any successive file reads unless\n",
    "            explicitly included.  A value of None, the default, will not\n",
    "            introduct any addition metadata and the file specific or default\n",
    "            metadata as specified by the Py-ART configuration file will be used.\n",
    "        file_field_names : bool, optional\n",
    "            True to use the MDV data type names for the field names. If this\n",
    "            case the field_names parameter is ignored. The field dictionary will\n",
    "            likely only have a 'data' key, unless the fields are defined in\n",
    "            `additional_metadata`.\n",
    "        exclude_fields : list or None, optional\n",
    "            List of fields to exclude from the radar object. This is applied\n",
    "            after the `file_field_names` and `field_names` parameters.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        radar : Radar\n",
    "            Radar object containing data from SINARAME_H5 file.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO before moving to pyart.io\n",
    "        # * unit test\n",
    "        # * add default field mapping, etc to default config\n",
    "        # * auto-detect file type with pyart.io.read function\n",
    "        # * instrument parameters\n",
    "        # * add additional checks for HOW attributes\n",
    "        # * support for other objects (SCAN, XSEC)\n",
    "\n",
    "        # check that h5py is available\n",
    "        if not _H5PY_AVAILABLE:\n",
    "            raise MissingOptionalDependency(\n",
    "                \"h5py is required to use read_sinarame_h5 but is not installed\")\n",
    "\n",
    "        # test for non empty kwargs\n",
    "        _test_arguments(kwargs)\n",
    "\n",
    "        # create metadata retrieval object\n",
    "        if field_names is None:\n",
    "            field_names = SINARAME_H5_FIELD_NAMES\n",
    "        filemetadata = FileMetadata('SINARAME_h5', field_names,\n",
    "                                    additional_metadata,\n",
    "                                    file_field_names, exclude_fields)\n",
    "\n",
    "        # open the file\n",
    "        hfile = h5py.File(filename, 'r')\n",
    "        SINARAME_fields = [os.path.basename(filename).split('_')[3]]\n",
    "        SINARAME_object = _to_str(hfile['what'].attrs['object'])\n",
    "        if SINARAME_object not in ['PVOL', 'SCAN', 'ELEV', 'AZIM']:\n",
    "            raise NotImplementedError(\n",
    "                'object: %s not implemented.' % (SINARAME_object))\n",
    "\n",
    "        # determine the number of sweeps by the number of groups which\n",
    "        # begin with dataset\n",
    "        datasets = [k for k in hfile if k.startswith('dataset')]\n",
    "        datasets.sort(key=lambda x: int(x[7:]))\n",
    "        nsweeps = len(datasets)\n",
    "\n",
    "        # latitude, longitude and altitude\n",
    "        latitude = filemetadata('latitude')\n",
    "        longitude = filemetadata('longitude')\n",
    "        altitude = filemetadata('altitude')\n",
    "\n",
    "        h_where = hfile['where'].attrs\n",
    "        latitude['data'] = np.array([h_where['lat']], dtype='float64')\n",
    "        longitude['data'] = np.array([h_where['lon']], dtype='float64')\n",
    "        altitude['data'] = np.array([h_where['height']], dtype='float64')\n",
    "\n",
    "        # metadata\n",
    "        metadata = filemetadata('metadata')\n",
    "        metadata['source'] = _to_str(hfile['what'].attrs['source'])\n",
    "        metadata['original_container'] = 'SINARAME_h5'\n",
    "        metadata['SINARAME_conventions'] = _to_str(hfile.attrs['Conventions'])\n",
    "\n",
    "        h_what = hfile['what'].attrs\n",
    "        metadata['version'] = _to_str(h_what['version'])\n",
    "        metadata['source'] = _to_str(h_what['source'])\n",
    "\n",
    "        try:\n",
    "            ds1_how = hfile[datasets[0]]['how'].attrs\n",
    "        except KeyError:\n",
    "            # if no how group exists mock it with an empty dictionary\n",
    "            ds1_how = {}\n",
    "        if 'system' in ds1_how:\n",
    "            metadata['system'] = ds1_how['system']\n",
    "        if 'software' in ds1_how:\n",
    "            metadata['software'] = ds1_how['software']\n",
    "        if 'sw_version' in ds1_how:\n",
    "            metadata['sw_version'] = ds1_how['sw_version']\n",
    "\n",
    "        # sweep_start_ray_index, sweep_end_ray_index\n",
    "        sweep_start_ray_index = filemetadata('sweep_start_ray_index')\n",
    "        sweep_end_ray_index = filemetadata('sweep_end_ray_index')\n",
    "\n",
    "        if SINARAME_object in ['AZIM', 'SCAN', 'PVOL']:\n",
    "            rays_per_sweep = [\n",
    "                int(hfile[d]['where'].attrs['nrays']) for d in datasets]\n",
    "        elif SINARAME_object == 'ELEV':\n",
    "            rays_per_sweep = [\n",
    "                int(hfile[d]['where'].attrs['angles'].size) for d in datasets]\n",
    "        total_rays = sum(rays_per_sweep)\n",
    "        ssri = np.cumsum(np.append([0], rays_per_sweep[:-1])).astype('int32')\n",
    "        seri = np.cumsum(rays_per_sweep).astype('int32') - 1\n",
    "        sweep_start_ray_index['data'] = ssri\n",
    "        sweep_end_ray_index['data'] = seri\n",
    "\n",
    "        # sweep_number\n",
    "        sweep_number = filemetadata('sweep_number')\n",
    "        sweep_number['data'] = np.arange(nsweeps, dtype='int32')\n",
    "\n",
    "        # sweep_mode\n",
    "        sweep_mode = filemetadata('sweep_mode')\n",
    "        sweep_mode['data'] = np.array(nsweeps * ['azimuth_surveillance'])\n",
    "\n",
    "        # scan_type\n",
    "        if SINARAME_object == 'ELEV':\n",
    "            scan_type = 'rhi'\n",
    "        else:\n",
    "            scan_type = 'ppi'\n",
    "\n",
    "        # fixed_angle\n",
    "        fixed_angle = filemetadata('fixed_angle')\n",
    "        if SINARAME_object == 'ELEV':\n",
    "            sweep_el = [hfile[d]['where'].attrs['az_angle'] for d in datasets]\n",
    "        else:\n",
    "            sweep_el = [hfile[d]['where'].attrs['elangle'] for d in datasets]\n",
    "        fixed_angle['data'] = np.array(sweep_el, dtype='float32')\n",
    "\n",
    "        # elevation\n",
    "        elevation = filemetadata('elevation')\n",
    "        if 'elangles' in ds1_how:\n",
    "            edata = np.empty(total_rays, dtype='float32')\n",
    "            for d, start, stop in zip(datasets, ssri, seri):\n",
    "                edata[start:stop+1] = hfile[d]['how'].attrs['elangles'][:]\n",
    "            elevation['data'] = edata\n",
    "        elif SINARAME_object == 'ELEV':\n",
    "            edata = np.empty(total_rays, dtype='float32')\n",
    "            for d, start, stop in zip(datasets, ssri, seri):\n",
    "                edata[start:stop+1] = hfile[d]['where'].attrs['angles'][:]\n",
    "            elevation['data'] = edata\n",
    "        else:\n",
    "            elevation['data'] = np.repeat(sweep_el, rays_per_sweep)\n",
    "\n",
    "        # range\n",
    "        _range = filemetadata('range')\n",
    "        if 'rstart' in hfile['dataset1/where'].attrs:\n",
    "            # derive range from rstart and rscale attributes if available\n",
    "\n",
    "            # check that the gate spacing is constant between sweeps\n",
    "            rstart = [hfile[d]['where'].attrs['rstart'] for d in datasets]\n",
    "            if any(rstart != rstart[0]):\n",
    "                raise ValueError('range start changes between sweeps')\n",
    "            rscale = [hfile[d]['where'].attrs['rscale'] for d in datasets]\n",
    "            if any(rscale != rscale[0]):\n",
    "                raise ValueError('range scale changes between sweeps')\n",
    "            nbins = int(hfile['dataset1']['where'].attrs['nbins'])\n",
    "            _range['data'] = (np.arange(nbins, dtype='float32') * rscale[0] +\n",
    "                              rstart[0])\n",
    "            _range['meters_to_center_of_first_gate'] = rstart[0]\n",
    "            _range['meters_between_gates'] = float(rscale[0])\n",
    "        else:\n",
    "            # if not defined use range attribute which defines the maximum range in\n",
    "            # km.  There is no information on the starting location of the range\n",
    "            # bins so we assume this to be 0.\n",
    "            # This most often occurs in RHI files, which technically do not meet\n",
    "            # the ODIM 2.2 specs. Section 7.4 requires that these files include\n",
    "            # the where/rstart, where/rscale and where/nbins attributes.\n",
    "            max_range = [hfile[d]['where'].attrs['range'] for d in datasets]\n",
    "            if any(max_range != max_range[0]):\n",
    "                raise ValueError('maximum range changes between sweeps')\n",
    "            # nbins is required\n",
    "            nbins = hfile['dataset1/data1/data'].shape[1]\n",
    "            _range['data'] = np.linspace(\n",
    "                0, max_range[0] * 1000., nbins, dtype='float32')\n",
    "            _range['meters_to_center_of_first_gate'] = 0\n",
    "            _range['meters_between_gates'] = max_range[0] * 1000. / nbins\n",
    "\n",
    "        # azimuth\n",
    "        azimuth = filemetadata('azimuth')\n",
    "        az_data = np.ones((total_rays, ), dtype='float32')\n",
    "        for dset, start, stop in zip(datasets, ssri, seri):\n",
    "            if SINARAME_object == 'ELEV':\n",
    "                # all azimuth angles are the sweep azimuth angle\n",
    "                sweep_az = hfile[dset]['where'].attrs['az_angle']\n",
    "            elif SINARAME_object == 'AZIM':\n",
    "                # Sector azimuths are specified in the startaz and stopaz\n",
    "                # attribute of dataset/where.\n",
    "                # Assume that the azimuth angles do not pass through 0/360 degrees\n",
    "                startaz = hfile[dset]['where'].attrs['startaz']\n",
    "                stopaz = hfile[dset]['where'].attrs['stopaz']\n",
    "                nrays = stop - start + 1\n",
    "                sweep_az = np.linspace(startaz, stopaz, nrays, endpoint=True)\n",
    "            elif ('startazA' in ds1_how) and ('stopazA' in ds1_how):\n",
    "                # average between start and stop azimuth angles\n",
    "                startaz = hfile[dset]['how'].attrs['startazA']\n",
    "                stopaz = hfile[dset]['how'].attrs['stopazA']\n",
    "                sweep_az = np.angle(\n",
    "                    (np.exp(1.j*np.deg2rad(startaz)) +\n",
    "                     np.exp(1.j*np.deg2rad(stopaz))) / 2., deg=True)\n",
    "            else:\n",
    "                # according to section 5.1 the first ray points north (0 degrees)\n",
    "                # and proceeds clockwise for a complete 360 rotation.\n",
    "                nrays = stop - start + 1\n",
    "                sweep_az = np.linspace(0, 360, nrays, endpoint=False)\n",
    "            az_data[start:stop+1] = sweep_az\n",
    "        azimuth['data'] = az_data\n",
    "\n",
    "        # time\n",
    "        _time = filemetadata('time')\n",
    "        if ('startazT' in ds1_how) and ('stopazT' in ds1_how):\n",
    "            # average between startazT and stopazT\n",
    "            t_data = np.empty((total_rays, ), dtype='float32')\n",
    "            for dset, start, stop in zip(datasets, ssri, seri):\n",
    "                t_start = hfile[dset]['how'].attrs['startazT']\n",
    "                t_stop = hfile[dset]['how'].attrs['stopazT']\n",
    "                t_data[start:stop+1] = (t_start + t_stop) / 2\n",
    "            start_epoch = t_data.min()\n",
    "            start_time = datetime.datetime.utcfromtimestamp(start_epoch)\n",
    "            _time['units'] = make_time_unit_str(start_time)\n",
    "            _time['data'] = t_data - start_epoch\n",
    "        else:\n",
    "            t_data = np.empty((total_rays, ), dtype='int32')\n",
    "            # interpolate between each sweep starting and ending time\n",
    "            for dset, start, stop in zip(datasets, ssri, seri):\n",
    "                dset_what = hfile[dset]['what'].attrs\n",
    "                start_str = _to_str(\n",
    "                    dset_what['startdate'] + dset_what['starttime'])\n",
    "                end_str = _to_str(dset_what['enddate'] + dset_what['endtime'])\n",
    "                start_dt = datetime.datetime.strptime(start_str, '%Y%m%d%H%M%S')\n",
    "                end_dt = datetime.datetime.strptime(end_str, '%Y%m%d%H%M%S')\n",
    "\n",
    "                time_delta = end_dt - start_dt\n",
    "                delta_seconds = time_delta.seconds + time_delta.days * 3600 * 24\n",
    "                rays = stop - start + 1\n",
    "                sweep_start_epoch = (\n",
    "                    start_dt - datetime.datetime(1970, 1, 1)).total_seconds()\n",
    "                t_data[start:stop+1] = (sweep_start_epoch +\n",
    "                                        np.linspace(0, delta_seconds, rays))\n",
    "            start_epoch = t_data.min()\n",
    "            start_time = datetime.datetime.utcfromtimestamp(start_epoch)\n",
    "            _time['units'] = make_time_unit_str(start_time)\n",
    "            _time['data'] = (t_data - start_epoch).astype('float32')\n",
    "\n",
    "        # fields\n",
    "        fields = {}\n",
    "        h_field_keys = [k for k in hfile['dataset1'] if k.startswith('data')]\n",
    "    #    SINARAME_fields = [hfile['dataset1'][d]['what'].attrs['quantity'] for d in\n",
    "    #                       h_field_keys]\n",
    "    #    print(SINARAME_fields, h_field_keys)\n",
    "        for SINARAME_field, h_field_key in zip(SINARAME_fields, h_field_keys):\n",
    "    #        field_name = filemetadata.get_field_name(_to_str(SINARAME_field))\n",
    "    #        print(field_name)\n",
    "    #        if field_name is None:\n",
    "    #            continue\n",
    "            field_name=SINARAME_field\n",
    "    #        print(SINARAME_field)\n",
    "            fdata = np.ma.zeros((total_rays, nbins), dtype='float32')\n",
    "            start = 0\n",
    "            # loop over the sweeps, copy data into correct location in data array\n",
    "            for dset, rays_in_sweep in zip(datasets, rays_per_sweep):\n",
    "                sweep_data = _get_SINARAME_h5_sweep_data(hfile[dset][h_field_key])\n",
    "                sweep_nbins = sweep_data.shape[1]\n",
    "                fdata[start:start + rays_in_sweep, :sweep_nbins] = sweep_data[:]\n",
    "                start += rays_in_sweep\n",
    "            # create field dictionary\n",
    "            field_dic = filemetadata(field_name)\n",
    "            field_dic['data'] = fdata\n",
    "            field_dic['_FillValue'] = get_fillvalue()\n",
    "            fields[field_name] = field_dic\n",
    "            if file_field_names:\n",
    "                fields[field_name].update(\n",
    "                    filemetadata.get_metadata(field_names[field_name]))\n",
    "\n",
    "        # instrument_parameters\n",
    "        instrument_parameters = None\n",
    "\n",
    "        return Radar(\n",
    "            _time, _range, fields, metadata, scan_type,\n",
    "            latitude, longitude, altitude,\n",
    "            sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index,\n",
    "            sweep_end_ray_index,\n",
    "            azimuth, elevation,\n",
    "            instrument_parameters=instrument_parameters)\n",
    "\n",
    "\n",
    "    def _to_str(text):\n",
    "        \"\"\" Convert bytes to str if necessary. \"\"\"\n",
    "        if hasattr(text, 'decode'):\n",
    "            return text.decode('utf-8')\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "\n",
    "    def _get_SINARAME_h5_sweep_data(group):\n",
    "        \"\"\" Get SINARAME_H5 sweet data from an HDF5 group. \"\"\"\n",
    "\n",
    "        # mask raw data\n",
    "        what = group['what']\n",
    "        raw_data = group['data'][:]\n",
    "\n",
    "        if 'nodata' in what.attrs:\n",
    "            nodata = what.attrs.get('nodata')\n",
    "            data = np.ma.masked_equal(raw_data, nodata)\n",
    "        else:\n",
    "            data = np.ma.masked_array(raw_data)\n",
    "        if 'undetect' in what.attrs:\n",
    "            undetect = what.attrs.get('undetect')\n",
    "            data[data == undetect] = np.ma.masked\n",
    "\n",
    "        offset = 0.0\n",
    "        gain = 1.0\n",
    "        if 'offset' in what.attrs:\n",
    "            offset = what.attrs.get('offset')\n",
    "        if 'gain' in what.attrs:\n",
    "            gain = what.attrs.get('gain')\n",
    "        return data * gain + offset\n",
    "\n",
    "    files=glob.glob(path+'/??/????/*Z.H5')\n",
    "    \n",
    "    for i in np.arange(len(files)):\n",
    "        basename=os.path.basename(files[i])\n",
    "        bs=basename.split('_')\n",
    "        field=bs[3]\n",
    "        radar=read_sinarame_h5(files[i],fields=SINARAME_H5_FIELD_NAMES)\n",
    "        \n",
    "        for j in range(radar.nsweeps):\n",
    "            iradar=radar.extract_sweeps([j])\n",
    "        \n",
    "            cdftime = utime(iradar.time['units'])\n",
    "        \n",
    "            time1=cdftime.num2date(iradar.time['data'][0]).strftime('%Y%m%d_%H%M%S')\n",
    "            time2=cdftime.num2date(iradar.time['data'][-1]).strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "            iradar._DeflateLevel=5\n",
    "            basename=os.path.basename(files[i])\n",
    "            bs=basename.split('_')\n",
    "            cffile='cfrad.{time1}.0000_to_{time2}.0000_{b3}_{b1}_{elev}_SUR.nc'.format(time1=time1,time2=time2,\n",
    "                                                                                       b1=field,b3=bs[0],\n",
    "                                                                                      elev='{:0.1f}'.format(float(iradar.fixed_angle['data'])))\n",
    "            print(\"writing \"+cffile)\n",
    "            write_cfradial(os.path.join(path,cffile),iradar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncMapResult: convert_h5_to_cfradial:finished>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob2\n",
    "\n",
    "#Create list of folders with filesystem wildcard\n",
    "folder_list = '/data/meso/a/snesbitt/RMA1-new/201?/??/??'\n",
    "\n",
    "files=np.array(glob2.glob(folder_list))\n",
    "convert_h5_to_cfradial.map(files)\n",
    "\n",
    "# If you don't have ipyparallel use the following instead\n",
    "#convert_h5_to_cfradial(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
